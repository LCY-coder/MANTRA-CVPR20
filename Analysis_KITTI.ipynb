{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# module\n",
    "import os\n",
    "import matplotlib.pylab as pl\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import cv2\n",
    "import numpy as np\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import dataset_invariance\n",
    "import index_qualitative\n",
    "from torch.autograd import Variable\n",
    "import csv\n",
    "import time\n",
    "import tqdm\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Scene colormap \n",
    "colors = [(0, 0, 0), (0.87, 0.87, 0.87), (0.54, 0.54, 0.54), (0.49, 0.33, 0.16), (0.29, 0.57, 0.25)]\n",
    "cmap_name = 'scene_cmap'\n",
    "cm = LinearSegmentedColormap.from_list(cmap_name, colors, N=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function\n",
    "def draw_track(past, future, pred=None, video_id='', vec_id='', index_tracklet=0):\n",
    "\n",
    "    fig = plt.figure()\n",
    "    plt.plot(past[:, 0], past[:, 1], c='blue', linewidth=1, marker='o', markersize=1)\n",
    "    plt.plot(future[:, 0], future[:, 1], c='green', linewidth=1, marker='o', markersize=1)\n",
    "    if pred is not None:\n",
    "        plt.plot(pred[:, 0], pred[:, 1], c='red', linewidth=1, marker='o', markersize=1)\n",
    "    plt.axis('equal')\n",
    "    plt.title(video_id + '_' + vec_id + '_' + str(index_tracklet).zfill(3))\n",
    "    plt.show()\n",
    "    plt.close(fig)\n",
    "\n",
    "def draw_track_with_MANTRA(past, future, preds=None, video_id='', vec_id='', index_tracklet=0):\n",
    "\n",
    "    fig = plt.figure()\n",
    "    plt.plot(past[:, 0], past[:, 1], c='blue', linewidth=1, marker='o', markersize=1)\n",
    "    plt.plot(future[:, 0], future[:, 1], c='green', linewidth=1, marker='o', markersize=1)\n",
    "    for p in range(preds.shape[0]):\n",
    "        plt.plot(preds[p, :, 0], preds[p, :, 1], c='red', linewidth=1, marker='o', markersize=1)\n",
    "    plt.axis('equal')\n",
    "    plt.title(video_id + '_' + vec_id + '_' + str(index_tracklet).zfill(3))\n",
    "    plt.show()\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "cos = torch.nn.CosineSimilarity(dim=1, eps=1e-08)\n",
    "batch_size = 1024\n",
    "past_len = 20\n",
    "future_len = 40\n",
    "dim_embedding_key = 48\n",
    "num_prediction = 5\n",
    "dim_clip = 180\n",
    "\n",
    "# Model\n",
    "mem_n2n = torch.load('pretrained_models/model_IRM_epoch_449',  map_location=torch.device('cpu')).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Dataset\n",
    "tracks = json.load(open('kitti_dataset.json'))\n",
    "print('creating dataset...')\n",
    "data_train = dataset_invariance.TrackDataset(tracks, len_past=past_len, len_future=future_len,\n",
    "                                             train=True, dim_clip=dim_clip)\n",
    "data_test = dataset_invariance.TrackDataset(tracks, len_past=past_len, len_future=future_len,\n",
    "                                     train=False, dim_clip=dim_clip)\n",
    "train_loader = DataLoader(data_train, batch_size=batch_size, num_workers=1, shuffle=True)\n",
    "test_loader = DataLoader(data_test, batch_size=batch_size, num_workers=1, shuffle=False)\n",
    "print('dataset created')\n",
    "\n",
    "# loader iterator\n",
    "dataiter_test = iter(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iteration (test)\n",
    "(index, past, future, presents, angle_presents, videos, vehicles, number_vec, scene, scene_one_hot) = dataiter_test.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Inference\n",
    "# if ablation_study is None, the inference is default.\n",
    "# if 'zeros', encoding of past is all zeros.\n",
    "# if 'rand', encoding of past is random numbers (standard normal distribution, mean 0, variance 1).\n",
    "# if 'different', encoding of all pasts (of batch) is the encoding of 'j_ablation' past.\n",
    "ablation_study = 'different'\n",
    "j_ablation = 802\n",
    "\n",
    "with torch.no_grad():\n",
    "    dim_batch = past.size()[0]\n",
    "    zero_padding = torch.zeros(1, dim_batch, mem_n2n.dim_embedding_key * 2)\n",
    "    prediction = torch.Tensor()\n",
    "    present = past[:, -1, :2].unsqueeze(1)\n",
    "\n",
    "    # temporal encoding for past\n",
    "    past_t = torch.transpose(past, 1, 2)\n",
    "    past_embed = mem_n2n.relu(mem_n2n.conv_past(past_t))\n",
    "    past_embed = torch.transpose(past_embed, 1, 2)\n",
    "\n",
    "    # temporal encoding for future\n",
    "    future_t = torch.transpose(future, 1, 2)\n",
    "    future_embed = mem_n2n.relu(mem_n2n.conv_fut(future_t))\n",
    "    future_embed = torch.transpose(future_embed, 1, 2)\n",
    "\n",
    "    # sequence encoding\n",
    "    output_past, state_past = mem_n2n.encoder_past(past_embed)\n",
    "    output_fut, state_fut = mem_n2n.encoder_fut(future_embed)\n",
    "\n",
    "    if ablation_study is not None:\n",
    "        if ablation_study is 'zeros':\n",
    "            state_past = torch.zeros([1, dim_batch, 48])\n",
    "        elif ablation_study is 'rand':\n",
    "            state_past = torch.randn([1, dim_batch, 48])\n",
    "        elif ablation_study is 'different':\n",
    "            state_past = state_past[:,j].unsqueeze(0).repeat(1, dim_batch,1)\n",
    "\n",
    "    # concatenate\n",
    "    state_conc = torch.cat((state_past, state_fut), 2)\n",
    "    input_fut = state_conc\n",
    "    state_fut = zero_padding\n",
    "    for i_pred in range(mem_n2n.future_len):\n",
    "        output_decoder, state_fut = mem_n2n.decoder(input_fut, state_fut)\n",
    "        displacement_next = mem_n2n.FC_output(output_decoder)\n",
    "        coords_next = present + displacement_next.squeeze(0).unsqueeze(1)\n",
    "        prediction = torch.cat((prediction, coords_next), 1)\n",
    "        present = coords_next\n",
    "        input_fut = zero_padding\n",
    "prediction = prediction.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "draw_track(past[i], future[i], prediction[i], video_id=videos[i], vec_id=vehicles[i] + number_vec[i], index_tracklet=index[i].item() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREDICTION FROM MEMORY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    dim_batch = past.size()[0]\n",
    "    zero_padding = torch.zeros(1, dim_batch * num_prediction, mem_n2n.dim_embedding_key * 2)\n",
    "    prediction_mantra = torch.Tensor()\n",
    "    present = past[:, -1].unsqueeze(1)\n",
    "\n",
    "    # past temporal encoding\n",
    "    past_t = torch.transpose(past, 1, 2)\n",
    "    story_embed = mem_n2n.relu(mem_n2n.conv_past(past_t))\n",
    "    story_embed = torch.transpose(story_embed, 1, 2)\n",
    "    output_past, state_past = mem_n2n.encoder_past(story_embed)\n",
    "\n",
    "    # Cosine similarity and memory read\n",
    "    past_normalized = F.normalize(mem_n2n.memory_past, p=2, dim=1)\n",
    "    state_normalized = F.normalize(state_past.squeeze(dim=0), p=2, dim=1)\n",
    "    weight_read = torch.matmul(past_normalized, state_normalized.transpose(0, 1)).transpose(0, 1)\n",
    "    index_max = torch.sort(weight_read, descending=True)[1].cpu()[:, :num_prediction]\n",
    "    present = present.repeat_interleave(num_prediction, dim=0)\n",
    "    state_past = state_past.repeat_interleave(num_prediction, dim=1)\n",
    "    ind = index_max.flatten()\n",
    "\n",
    "\n",
    "    info_future = mem_n2n.memory_fut[ind]\n",
    "    info_total = torch.cat((state_past, info_future.unsqueeze(0)), 2)\n",
    "    input_dec = info_total\n",
    "    state_dec = zero_padding\n",
    "    for i in range(future_len):\n",
    "        output_decoder, state_dec = mem_n2n.decoder(input_dec, state_dec)\n",
    "        displacement_next = mem_n2n.FC_output(output_decoder)\n",
    "        coords_next = present + displacement_next.squeeze(0).unsqueeze(1)\n",
    "        prediction_mantra = torch.cat((prediction_mantra, coords_next), 1)\n",
    "        present = coords_next\n",
    "        input_dec = zero_padding\n",
    "    prediction_mantra = prediction_mantra.view(dim_batch, num_prediction, future_len, 2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "draw_track_with_MANTRA(past[i], future[i], prediction_mantra[i], video_id=videos[i], vec_id=vehicles[i] + number_vec[i], index_tracklet=index[i].item() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
