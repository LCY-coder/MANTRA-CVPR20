{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# module\n",
    "import os\n",
    "import matplotlib.pylab as pl\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import cv2\n",
    "import numpy as np\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import dataset_invariance\n",
    "import index_qualitative\n",
    "from torch.autograd import Variable\n",
    "import csv\n",
    "import time\n",
    "import tqdm\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Scene colormap \n",
    "colors = [(0, 0, 0), (0.87, 0.87, 0.87), (0.54, 0.54, 0.54), (0.49, 0.33, 0.16), (0.29, 0.57, 0.25)]\n",
    "cmap_name = 'scene_cmap'\n",
    "cm = LinearSegmentedColormap.from_list(cmap_name, colors, N=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function\n",
    "def draw_track_autoencoder(past, future, pred=None, video_id='', vec_id='', index_tracklet=0):\n",
    "\n",
    "    fig = plt.figure()\n",
    "    plt.plot(past[:, 0], past[:, 1], c='blue', linewidth=1, marker='o', markersize=1)\n",
    "    plt.plot(future[:, 0], future[:, 1], c='green', linewidth=1, marker='o', markersize=1)\n",
    "    if pred is not None:\n",
    "        plt.plot(pred[:, 0], pred[:, 1], c='red', linewidth=1, marker='o', markersize=1)\n",
    "    plt.axis('equal')\n",
    "    plt.title(video_id + '_' + vec_id + '_' + str(index_tracklet).zfill(3))\n",
    "    plt.show()\n",
    "    plt.close(fig)\n",
    "\n",
    "def draw_predictions(past, future, scene_track, pred=None, angle=0, video_id='', vec_id='', index_tracklet=0,\n",
    "                horizon_dist=None):\n",
    "    \"\"\"\n",
    "    Plot past and future trajectory and save it to test folder.\n",
    "    :param past: the observed trajectory\n",
    "    :param future: ground truth future trajectory\n",
    "    :param pred: predicted future trajectory\n",
    "    :param angle: rotation angle to plot the trajectory in the original direction\n",
    "    :param video_id: video index of the trajectory\n",
    "    :param vec_id: vehicle type of the trajectory\n",
    "    :param pred: predicted future trajectory\n",
    "    :param: the observed scene where is the trajectory\n",
    "    :param index_tracklet: index of the trajectory in the dataset (default 0)\n",
    "    :param num_epoch: current epoch (default 0)\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "\n",
    "    colors = [(0, 0, 0), (0.87, 0.87, 0.87), (0.54, 0.54, 0.54), (0.49, 0.33, 0.16), (0.29, 0.57, 0.25)]\n",
    "    cmap_name = 'scene_cmap'\n",
    "    cm = LinearSegmentedColormap.from_list(cmap_name, colors, N=5)\n",
    "    fig = plt.figure()\n",
    "    plt.imshow(scene_track, cmap=cm)\n",
    "    colors = pl.cm.Reds(np.linspace(1, 0.3, pred.shape[0]))\n",
    "\n",
    "    matRot_track = cv2.getRotationMatrix2D((0, 0), -angle, 1)\n",
    "    past = cv2.transform(past.numpy().reshape(-1, 1, 2), matRot_track).squeeze()\n",
    "    future = cv2.transform(future.numpy().reshape(-1, 1, 2), matRot_track).squeeze()\n",
    "    story_scene = past * 2 + dim_clip\n",
    "    future_scene = future * 2 + dim_clip\n",
    "    plt.plot(story_scene[:, 0], story_scene[:, 1], c='blue', linewidth=1, marker='o', markersize=1)\n",
    "    if pred is not None:\n",
    "        for i_p in reversed(range(pred.shape[0])):\n",
    "            pred_i = cv2.transform(pred[i_p].numpy().reshape(-1, 1, 2), matRot_track).squeeze()\n",
    "            pred_scene = pred_i * 2 + dim_clip\n",
    "            plt.plot(pred_scene[:, 0], pred_scene[:, 1], color=colors[i_p], linewidth=0.5, marker='o', markersize=0.5)\n",
    "    plt.plot(future_scene[:, 0], future_scene[:, 1], c='green', linewidth=1, marker='o', markersize=1)\n",
    "    plt.title('FDE 1s: ' + str(horizon_dist[0]) + ' FDE 2s: ' + str(horizon_dist[1]) + ' FDE 3s: ' +\n",
    "              str(horizon_dist[2]) + ' FDE 4s: ' + str(horizon_dist[3]))\n",
    "    plt.axis('equal')\n",
    "    plt.show()\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/delorean/fmarchetti/anaconda3/envs/myenv/lib/python3.7/site-packages/torch/serialization.py:493: SourceChangeWarning: source code of class 'models.model_memory_IRM.model_memory_IRM' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/delorean/fmarchetti/anaconda3/envs/myenv/lib/python3.7/site-packages/torch/serialization.py:493: SourceChangeWarning: source code of class 'torch.nn.modules.distance.CosineSimilarity' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/delorean/fmarchetti/anaconda3/envs/myenv/lib/python3.7/site-packages/torch/serialization.py:493: SourceChangeWarning: source code of class 'torch.nn.modules.conv.Conv1d' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/delorean/fmarchetti/anaconda3/envs/myenv/lib/python3.7/site-packages/torch/serialization.py:493: SourceChangeWarning: source code of class 'torch.nn.modules.rnn.GRU' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/delorean/fmarchetti/anaconda3/envs/myenv/lib/python3.7/site-packages/torch/serialization.py:493: SourceChangeWarning: source code of class 'torch.nn.modules.linear.Linear' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/delorean/fmarchetti/anaconda3/envs/myenv/lib/python3.7/site-packages/torch/serialization.py:493: SourceChangeWarning: source code of class 'torch.nn.modules.activation.ReLU' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/delorean/fmarchetti/anaconda3/envs/myenv/lib/python3.7/site-packages/torch/serialization.py:493: SourceChangeWarning: source code of class 'torch.nn.modules.activation.Softmax' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/delorean/fmarchetti/anaconda3/envs/myenv/lib/python3.7/site-packages/torch/serialization.py:493: SourceChangeWarning: source code of class 'torch.nn.modules.pooling.MaxPool2d' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/delorean/fmarchetti/anaconda3/envs/myenv/lib/python3.7/site-packages/torch/serialization.py:493: SourceChangeWarning: source code of class 'torch.nn.modules.conv.Conv2d' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/delorean/fmarchetti/anaconda3/envs/myenv/lib/python3.7/site-packages/torch/serialization.py:493: SourceChangeWarning: source code of class 'torch.nn.modules.batchnorm.BatchNorm2d' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "cos = torch.nn.CosineSimilarity(dim=1, eps=1e-08)\n",
    "batch_size = 256\n",
    "past_len = 20\n",
    "future_len = 40\n",
    "dim_embedding_key = 48\n",
    "num_prediction = 5\n",
    "dim_clip = 180\n",
    "\n",
    "# Model\n",
    "mem_n2n = torch.load('pretrained_models/MANTRA/model_MANTRA',  map_location=torch.device('cpu')).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating dataset...\n",
      "video: 0005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/delorean/fmarchetti/git/memnet_trajectory/dataset_invariance.py:191: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return vector / np.linalg.norm(vector)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "video: 0009\n",
      "video: 0011\n",
      "video: 0013\n",
      "video: 0014\n",
      "video: 0017\n",
      "video: 0027\n",
      "video: 0028\n",
      "video: 0048\n",
      "video: 0051\n",
      "video: 0056\n",
      "video: 0057\n",
      "video: 0059\n",
      "video: 0060\n",
      "video: 0084\n",
      "video: 0091\n",
      "video: 0001\n",
      "video: 0002\n",
      "video: 0015\n",
      "video: 0018\n",
      "video: 0029\n",
      "video: 0032\n",
      "video: 0052\n",
      "video: 0070\n",
      "dataset created\n"
     ]
    }
   ],
   "source": [
    "# Dataset\n",
    "tracks = json.load(open('kitti_dataset.json'))\n",
    "print('creating dataset...')\n",
    "data_train = dataset_invariance.TrackDataset(tracks, len_past=past_len, len_future=future_len,\n",
    "                                             train=True, dim_clip=dim_clip)\n",
    "data_test = dataset_invariance.TrackDataset(tracks, len_past=past_len, len_future=future_len,\n",
    "                                     train=False, dim_clip=dim_clip)\n",
    "train_loader = DataLoader(data_train, batch_size=batch_size, num_workers=1, shuffle=True)\n",
    "test_loader = DataLoader(data_test, batch_size=batch_size, num_workers=1, shuffle=False)\n",
    "print('dataset created')\n",
    "\n",
    "# loader iterator (test)\n",
    "dataiter_test = iter(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iteration (test)\n",
    "(index, past, future, presents, angle_presents, videos, vehicles, number_vec, scene, scene_one_hot) = dataiter_test.next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AUTOENCODER AND ABLATION STUDY (IMPORTANTE OF PAST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Inference\n",
    "# if ablation_study is None, the inference is default.\n",
    "# if 'zeros', encoding of past is all zeros.\n",
    "# if 'rand', encoding of past is random numbers (standard normal distribution, mean 0, variance 1).\n",
    "# if 'different', encoding of all pasts (of batch) is the encoding of 'j_ablation' past.\n",
    "ablation_study = None\n",
    "j_ablation = 802\n",
    "\n",
    "with torch.no_grad():\n",
    "    dim_batch = past.size()[0]\n",
    "    zero_padding = torch.zeros(1, dim_batch, mem_n2n.dim_embedding_key * 2)\n",
    "    reconstruction = torch.Tensor()\n",
    "    present = past[:, -1, :2].unsqueeze(1)\n",
    "\n",
    "    # temporal encoding for past\n",
    "    past_t = torch.transpose(past, 1, 2)\n",
    "    past_embed = mem_n2n.relu(mem_n2n.conv_past(past_t))\n",
    "    past_embed = torch.transpose(past_embed, 1, 2)\n",
    "\n",
    "    # temporal encoding for future\n",
    "    future_t = torch.transpose(future, 1, 2)\n",
    "    future_embed = mem_n2n.relu(mem_n2n.conv_fut(future_t))\n",
    "    future_embed = torch.transpose(future_embed, 1, 2)\n",
    "\n",
    "    # sequence encoding\n",
    "    output_past, state_past = mem_n2n.encoder_past(past_embed)\n",
    "    output_fut, state_fut = mem_n2n.encoder_fut(future_embed)\n",
    "\n",
    "    if ablation_study is not None:\n",
    "        if ablation_study is 'zeros':\n",
    "            state_past = torch.zeros([1, dim_batch, 48])\n",
    "        elif ablation_study is 'rand':\n",
    "            state_past = torch.randn([1, dim_batch, 48])\n",
    "        elif ablation_study is 'different':\n",
    "            state_past = state_past[:,j_ablation].unsqueeze(0).repeat(1, dim_batch,1)\n",
    "\n",
    "    # concatenate\n",
    "    state_conc = torch.cat((state_past, state_fut), 2)\n",
    "    input_fut = state_conc\n",
    "    state_fut = zero_padding\n",
    "    for i_pred in range(mem_n2n.future_len):\n",
    "        output_decoder, state_fut = mem_n2n.decoder(input_fut, state_fut)\n",
    "        displacement_next = mem_n2n.FC_output(output_decoder)\n",
    "        coords_next = present + displacement_next.squeeze(0).unsqueeze(1)\n",
    "        reconstruction = torch.cat((reconstruction, coords_next), 1)\n",
    "        present = coords_next\n",
    "        input_fut = zero_padding\n",
    "    reconstruction = reconstruction.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "t = 0\n",
    "draw_track_autoencoder(past[t], future[t], reconstruction[t], video_id=videos[t], vec_id=vehicles[t] + number_vec[t], index_tracklet=index[t].item() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREDICTIONS FROM MEMORY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MANTRA INFERENCE\n",
    "\n",
    "scene_mantra = scene_one_hot.clone()\n",
    "with torch.no_grad():\n",
    "    dim_batch = past.size()[0]\n",
    "    zero_padding = torch.zeros(1, dim_batch * num_prediction, mem_n2n.dim_embedding_key * 2)\n",
    "    prediction_mantra = torch.Tensor()\n",
    "    present = past[:, -1].unsqueeze(1)\n",
    "\n",
    "    # past temporal encoding\n",
    "    past_t = torch.transpose(past, 1, 2)\n",
    "    story_embed = mem_n2n.relu(mem_n2n.conv_past(past_t))\n",
    "    story_embed = torch.transpose(story_embed, 1, 2)\n",
    "    output_past, state_past = mem_n2n.encoder_past(story_embed)\n",
    "\n",
    "    # Cosine similarity and memory read\n",
    "    past_normalized = F.normalize(mem_n2n.memory_past, p=2, dim=1)\n",
    "    state_normalized = F.normalize(state_past.squeeze(dim=0), p=2, dim=1)\n",
    "    weight_read = torch.matmul(past_normalized, state_normalized.transpose(0, 1)).transpose(0, 1)\n",
    "    index_max = torch.sort(weight_read, descending=True)[1].cpu()[:, :num_prediction]\n",
    "    present = present.repeat_interleave(num_prediction, dim=0)\n",
    "    state_past = state_past.repeat_interleave(num_prediction, dim=1)\n",
    "    ind = index_max.flatten()\n",
    "\n",
    "    info_future = mem_n2n.memory_fut[ind]\n",
    "    info_total = torch.cat((state_past, info_future.unsqueeze(0)), 2)\n",
    "    input_dec = info_total\n",
    "    state_dec = zero_padding\n",
    "    for i in range(future_len):\n",
    "        output_decoder, state_dec = mem_n2n.decoder(input_dec, state_dec)\n",
    "        displacement_next = mem_n2n.FC_output(output_decoder)\n",
    "        coords_next = present + displacement_next.squeeze(0).unsqueeze(1)\n",
    "        prediction_mantra = torch.cat((prediction_mantra, coords_next), 1)\n",
    "        present = coords_next\n",
    "        input_dec = zero_padding\n",
    "    \n",
    "    scene_mantra = scene_mantra.permute(0, 3, 1, 2)\n",
    "    scene_1 = mem_n2n.convScene_1(scene_mantra)\n",
    "    scene_2 = mem_n2n.convScene_2(scene_1)\n",
    "    scene_2 = scene_2.repeat_interleave(num_prediction, dim=0)\n",
    "\n",
    "    # Iteratively refine predictions using context\n",
    "    for i_refine in range(4):\n",
    "        pred_map = prediction_mantra + 90\n",
    "        pred_map = pred_map.unsqueeze(2)\n",
    "        indices = pred_map.permute(0, 2, 1, 3)\n",
    "        # rescale between -1 and 1\n",
    "        indices = 2 * (indices / 180) - 1\n",
    "        output = F.grid_sample(scene_2, indices, mode='nearest')\n",
    "        output = output.squeeze(2).permute(0, 2, 1)\n",
    "\n",
    "        state_rnn = state_past\n",
    "        output_rnn, state_rnn = mem_n2n.RNN_scene(output, state_rnn)\n",
    "        prediction_refine = mem_n2n.fc_refine(state_rnn).view(-1, future_len, 2)\n",
    "        prediction_mantra = prediction_mantra + prediction_refine\n",
    "\n",
    "    prediction_mantra = prediction_mantra.view(dim_batch, num_prediction, future_len, 2)\n",
    "\n",
    "future_rep = future.unsqueeze(1).repeat(1, num_prediction, 1, 1)\n",
    "distances = torch.norm(prediction_mantra - future_rep, dim=3)\n",
    "mean_distances = torch.mean(distances, dim=2)\n",
    "index_min = torch.argmin(mean_distances, dim=1)\n",
    "distance_pred = distances[torch.arange(0, len(index_min)), index_min]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t = 100\n",
    "horizon_dist = [round(distance_pred[t, 9].item(), 3), round(distance_pred[t, 19].item(), 3),\n",
    "                round(distance_pred[t, 29].item(), 3), round(distance_pred[t, 39].item(), 3)]\n",
    "draw_track(past[t], future[t], scene[t], prediction_mantra[t], angle_presents[t], videos[t], vehicles[t] + number_vec[t],\n",
    "                                            index_tracklet=index[t], horizon_dist=horizon_dist)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
